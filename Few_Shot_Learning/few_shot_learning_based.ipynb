{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of modifications applied to improve the accuracy of Queryset \n",
    "- Add an additional layer to the Siamese network and include batch normalization and dropout layers to reduce the likelihood of overfitting.\n",
    "- Completely change the data loading process so that pairs are generated in batches and memory is released to prevent kernel crashes.\n",
    "- Use early stopping and an adaptive learning rate `ReduceLROnPlateau` to achieve a robust training pipeline.\n",
    "- Increase the number of shots in the support set.\n",
    "- Implement contrastive loss; however, the results were unsatisfactory compared to the previous approach, so it was ignored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 105  # Image size for resizing\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare Omniglot dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_omniglot():\n",
    "    \"\"\"\n",
    "    Load the Omniglot dataset using TFDS.\n",
    "    \"\"\"\n",
    "    (train_ds, test_ds), ds_info = tfds.load(\n",
    "        'omniglot',\n",
    "        split=['train', 'test'],\n",
    "        as_supervised=True,\n",
    "        with_info=True\n",
    "    )\n",
    "    return train_ds, test_ds, ds_info\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    \"\"\"\n",
    "    Resize and normalize the image.\n",
    "    \"\"\"\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset using TFDS\n",
    "train_ds, test_ds, ds_info = load_omniglot()\n",
    "train_ds = train_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.cache()\n",
    "train_ds = train_ds.shuffle(buffer_size=1000)\n",
    "# Batch the dataset so that we process one batch at a time (memory is released after each batch)\n",
    "train_ds = train_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Pair Generation ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_pairs_from_batch(images, labels):\n",
    "    \"\"\"\n",
    "    Given a batch of images and labels (as NumPy arrays), generate pairs:\n",
    "      - For each image, if possible, generate one positive pair (another image in the batch with the same label)\n",
    "      - And one negative pair (an image with a different label)\n",
    "    Returns:\n",
    "        X1, X2: Arrays of images for the two branches (shape: [num_pairs, IMG_SIZE, IMG_SIZE, 3])\n",
    "        pair_labels: Array of binary labels (1 for positive pairs, 0 for negative pairs)\n",
    "    \"\"\"\n",
    "    images = images.numpy()  # Convert to numpy\n",
    "    labels = labels.numpy()\n",
    "    batch_pairs = []\n",
    "    batch_pair_labels = []\n",
    "    for i in range(len(images)):\n",
    "        current_img = images[i]\n",
    "        current_label = labels[i]\n",
    "        # Positive pair: find another image in the batch with the same label\n",
    "        pos_indices = np.where(labels == current_label)[0]\n",
    "        if len(pos_indices) > 1:\n",
    "            # Exclude the current index\n",
    "            pos_indices = pos_indices[pos_indices != i]\n",
    "            pos_idx = np.random.choice(pos_indices)\n",
    "            pos_img = images[pos_idx]\n",
    "            batch_pairs.append([current_img, pos_img])\n",
    "            batch_pair_labels.append(1)\n",
    "        # Negative pair: choose an image with a different label\n",
    "        neg_indices = np.where(labels != current_label)[0]\n",
    "        if len(neg_indices) > 0:\n",
    "            neg_idx = np.random.choice(neg_indices)\n",
    "            neg_img = images[neg_idx]\n",
    "            batch_pairs.append([current_img, neg_img])\n",
    "            batch_pair_labels.append(0)\n",
    "    if len(batch_pairs) == 0:\n",
    "        print(\"No pairs are generated\")\n",
    "        # In case no pairs are generated, return empty arrays\n",
    "        return (np.empty((0, IMG_SIZE, IMG_SIZE, 3), dtype=np.float32),\n",
    "                np.empty((0, IMG_SIZE, IMG_SIZE, 3), dtype=np.float32),\n",
    "                np.empty((0,), dtype=np.int32))\n",
    "    # Stack pairs into a uniform array\n",
    "    pairs_array = np.stack(batch_pairs, axis=0)  # Shape: (num_pairs, 2, IMG_SIZE, IMG_SIZE, 3)\n",
    "    X1 = pairs_array[:, 0]\n",
    "    X2 = pairs_array[:, 1]\n",
    "    return X1.astype(np.float32), X2.astype(np.float32), np.array(batch_pair_labels, dtype=np.int32)\n",
    "\n",
    "def pair_generator_from_batch(images, labels):\n",
    "    \"\"\"\n",
    "    Wrap generate_pairs_from_batch into a function that can be mapped over a tf.data.Dataset.\n",
    "    \"\"\"\n",
    "    X1, X2, pair_labels = tf.py_function(\n",
    "        func=generate_pairs_from_batch,\n",
    "        inp=[images, labels],\n",
    "        Tout=[tf.float32, tf.float32, tf.int32]\n",
    "    )\n",
    "    # Set static shapes. The first dimension is unknown (None) because it depends on the batch.\n",
    "    X1.set_shape([None, IMG_SIZE, IMG_SIZE, 3])\n",
    "    X2.set_shape([None, IMG_SIZE, IMG_SIZE, 3])\n",
    "    pair_labels.set_shape([None])\n",
    "    return (X1, X2), pair_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the pairing function onto each batch.\n",
    "pairs_ds = train_ds.map(pair_generator_from_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# Filter out any batches that did not generate pairs\n",
    "pairs_ds = pairs_ds.filter(lambda x, y: tf.shape(x[0])[0] > 0)\n",
    "pairs_ds = pairs_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Build the Siamese Network ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arash/anaconda3/envs/MA_Arash/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def build_feature_extractor():\n",
    "    \"\"\"\n",
    "    Build the convolutional feature extractor.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_siamese_network():\n",
    "    \"\"\"\n",
    "    Build and compile the Siamese network.\n",
    "    \"\"\"\n",
    "    feature_extractor = build_feature_extractor()\n",
    "    input_a = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    input_b = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    feat_a = feature_extractor(input_a)\n",
    "    feat_b = feature_extractor(input_b)\n",
    "    \n",
    "    # Compute L1 (absolute) distance between features\n",
    "    l1_distance = layers.Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))([feat_a, feat_b])\n",
    "    x = layers.Dense(64, activation='relu')(l1_distance)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    siamese_network = models.Model(inputs=[input_a, input_b], outputs=output)\n",
    "    siamese_network.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return siamese_network\n",
    "\n",
    "siamese_network = build_siamese_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally implement contrastive loss. howver, the accuracy didnot changed that much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    \n",
    "    Arguments:\n",
    "      y_true: 1 if the pair is similar, 0 if dissimilar.\n",
    "      y_pred: Euclidean distance between the two feature embeddings.\n",
    "      margin: The margin value for dissimilar pairs.\n",
    "    \n",
    "    Returns:\n",
    "      A scalar loss value.\n",
    "    \"\"\"\n",
    "    squared_pred = tf.square(y_pred)\n",
    "    margin_squared = tf.square(tf.maximum(margin - y_pred, 0))\n",
    "    return tf.reduce_mean(y_true * squared_pred + (1 - y_true) * margin_squared)\n",
    "\n",
    "def build_siamese_network2():\n",
    "    \"\"\"\n",
    "    Build and compile the Siamese network using contrastive loss.\n",
    "    \"\"\"\n",
    "    feature_extractor = build_feature_extractor()\n",
    "    \n",
    "    # Define two inputs\n",
    "    input_a = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    input_b = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    # Extract features from both inputs\n",
    "    feat_a = feature_extractor(input_a)\n",
    "    feat_b = feature_extractor(input_b)\n",
    "    \n",
    "    # Compute Euclidean distance between the two feature vectors\n",
    "    distance = layers.Lambda(\n",
    "        lambda tensors: tf.sqrt(tf.reduce_sum(tf.square(tensors[0] - tensors[1]), axis=1, keepdims=True))\n",
    "    )([feat_a, feat_b])\n",
    "    \n",
    "    # Create the Siamese network model\n",
    "    siamese_network = models.Model(inputs=[input_a, input_b], outputs=distance)\n",
    "    \n",
    "    # Compile the network with contrastive loss\n",
    "    siamese_network.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=contrastive_loss\n",
    "    )\n",
    "    return siamese_network\n",
    "\n",
    "siamese_network2 = build_siamese_network2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === Model Training ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.5125 - loss: 0.9951"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 171ms/step - accuracy: 0.5124 - loss: 0.9949 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.5255 - loss: 0.9248"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 158ms/step - accuracy: 0.5256 - loss: 0.9248 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.5536 - loss: 0.8660"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 162ms/step - accuracy: 0.5536 - loss: 0.8658 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.5639 - loss: 0.8109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 149ms/step - accuracy: 0.5639 - loss: 0.8108 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.5958 - loss: 0.7327"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 161ms/step - accuracy: 0.5958 - loss: 0.7328 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.6114 - loss: 0.7130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 144ms/step - accuracy: 0.6115 - loss: 0.7128 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6468 - loss: 0.7267 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arash/anaconda3/envs/MA_Arash/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.6375 - loss: 0.6766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 149ms/step - accuracy: 0.6376 - loss: 0.6764 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.6605 - loss: 0.6348"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 128ms/step - accuracy: 0.6605 - loss: 0.6348 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.6826 - loss: 0.5957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 144ms/step - accuracy: 0.6826 - loss: 0.5957 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.7166 - loss: 0.5342"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 139ms/step - accuracy: 0.7166 - loss: 0.5343 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.7329 - loss: 0.5353"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.7330 - loss: 0.5352 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.7649 - loss: 0.4822"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 131ms/step - accuracy: 0.7649 - loss: 0.4821 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m  3/100\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 194ms/step - accuracy: 0.7024 - loss: 0.4848"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7190 - loss: 0.4692 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 132ms/step - accuracy: 0.7797 - loss: 0.4626 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.7949 - loss: 0.4431"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 144ms/step - accuracy: 0.7950 - loss: 0.4432 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.8211 - loss: 0.4173"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 158ms/step - accuracy: 0.8209 - loss: 0.4174 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.8316 - loss: 0.4158"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 158ms/step - accuracy: 0.8316 - loss: 0.4156 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.8498 - loss: 0.3885"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 186ms/step - accuracy: 0.8498 - loss: 0.3885 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.8730 - loss: 0.3454"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 177ms/step - accuracy: 0.8730 - loss: 0.3454 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m  3/100\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 147ms/step - accuracy: 0.8948 - loss: 0.3365"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8877 - loss: 0.3434 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.8670 - loss: 0.3579"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 168ms/step - accuracy: 0.8671 - loss: 0.3577 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.8921 - loss: 0.3289"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 165ms/step - accuracy: 0.8921 - loss: 0.3289 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.8902 - loss: 0.3263"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 185ms/step - accuracy: 0.8902 - loss: 0.3262 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8897 - loss: 0.3253"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 183ms/step - accuracy: 0.8897 - loss: 0.3252 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.9092 - loss: 0.2869"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 172ms/step - accuracy: 0.9092 - loss: 0.2867 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.9249 - loss: 0.2786"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 185ms/step - accuracy: 0.9249 - loss: 0.2784 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m  3/100\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 141ms/step - accuracy: 0.9771 - loss: 0.2043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9629 - loss: 0.2134 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 166ms/step - accuracy: 0.9184 - loss: 0.2720 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 167ms/step - accuracy: 0.9233 - loss: 0.2654 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 168ms/step - accuracy: 0.9395 - loss: 0.2371 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 190ms/step - accuracy: 0.9307 - loss: 0.2432 - learning_rate: 5.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 174ms/step - accuracy: 0.9427 - loss: 0.2324 - learning_rate: 5.0000e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"siamese_best_model.h5\", monitor='loss', save_best_only=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "history = siamese_network.fit(\n",
    "    pairs_ds,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=100,  \n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---Inference---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Few-shot parameters\n",
    "NUM_WAYS = 3     # 3 classes per episode\n",
    "NUM_SHOTS = 3    # 3 examples per class for support set\n",
    "NUM_QUERIES = 3  # 3 query examples per class\n",
    "\n",
    "NUM_EPISODES = 25  # Evaluate over 25 episodes\n",
    "\n",
    "siamese_net = siamese_network  \n",
    "#siamese_net = siamese_network2  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(NUM_WAYS = 3, NUM_SHOTS = 3, NUM_QUERIES = 3):\n",
    "    \"\"\"\n",
    "    Create one episode with random support and query sets from Omniglot.\n",
    "    Returns: support_images, support_labels, query_images, query_labels\n",
    "    \"\"\"\n",
    "    ds = tfds.load('omniglot', split='test', as_supervised=True)\n",
    "    ds = ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.cache()  \n",
    "\n",
    "    # Convert dataset to a list for grouping \n",
    "    all_examples = list(tfds.as_numpy(ds))\n",
    "    label_to_images = defaultdict(list)\n",
    "    for img, label in all_examples:\n",
    "        label_to_images[label].append(img)\n",
    "    \n",
    "    all_labels = list(label_to_images.keys())\n",
    "    np.random.shuffle(all_labels)\n",
    "    selected_labels = all_labels[:NUM_WAYS]\n",
    "    \n",
    "    support_images, support_labels = [], []\n",
    "    query_images, query_labels = [], []\n",
    "    \n",
    "    for new_label, orig_label in enumerate(selected_labels):\n",
    "        imgs = label_to_images[orig_label]\n",
    "        np.random.shuffle(imgs)\n",
    "        support = imgs[:NUM_SHOTS]\n",
    "        query = imgs[NUM_SHOTS:NUM_SHOTS + NUM_QUERIES]\n",
    "        support_images.extend(support)\n",
    "        query_images.extend(query)\n",
    "        support_labels.extend([new_label] * NUM_SHOTS)\n",
    "        query_labels.extend([new_label] * NUM_QUERIES)\n",
    "    \n",
    "    support_images = np.stack(support_images)\n",
    "    query_images = np.stack(query_images)\n",
    "    support_labels = np.array(support_labels)\n",
    "    query_labels = np.array(query_labels)\n",
    "    \n",
    "    return support_images, support_labels, query_images, query_labels\n",
    "\n",
    "\n",
    "def classify_images(support_images, query_images, model, num_ways=NUM_WAYS, num_shots=NUM_SHOTS):\n",
    "    predictions = []\n",
    "    for query_img in query_images:\n",
    "        query_img_expanded = tf.expand_dims(query_img, axis=0)\n",
    "        tiled_query = np.tile(query_img_expanded, (support_images.shape[0], 1, 1, 1))\n",
    "        scores = model.predict([tiled_query, support_images], verbose=0)\n",
    "        aggregated_scores = np.zeros(num_ways)\n",
    "        for i in range(num_ways):\n",
    "            aggregated_scores[i] = np.mean(scores[i * num_shots:(i + 1) * num_shots])\n",
    "        predicted_class = np.argmax(aggregated_scores)\n",
    "        predictions.append(predicted_class)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Accuracy = 100.00%\n",
      "Episode 2: Accuracy = 100.00%\n",
      "Episode 3: Accuracy = 77.78%\n",
      "Episode 4: Accuracy = 55.56%\n",
      "Episode 5: Accuracy = 77.78%\n",
      "Episode 6: Accuracy = 66.67%\n",
      "Episode 7: Accuracy = 88.89%\n",
      "Episode 8: Accuracy = 88.89%\n",
      "Episode 9: Accuracy = 100.00%\n",
      "Episode 10: Accuracy = 88.89%\n",
      "Episode 11: Accuracy = 100.00%\n",
      "Episode 12: Accuracy = 88.89%\n",
      "Episode 13: Accuracy = 88.89%\n",
      "Episode 14: Accuracy = 55.56%\n",
      "Episode 15: Accuracy = 100.00%\n",
      "Episode 16: Accuracy = 100.00%\n",
      "Episode 17: Accuracy = 100.00%\n",
      "Episode 18: Accuracy = 88.89%\n",
      "Episode 19: Accuracy = 88.89%\n",
      "Episode 20: Accuracy = 100.00%\n",
      "Episode 21: Accuracy = 88.89%\n",
      "Episode 22: Accuracy = 77.78%\n",
      "Episode 23: Accuracy = 77.78%\n",
      "Episode 24: Accuracy = 88.89%\n",
      "Episode 25: Accuracy = 100.00%\n",
      "\n",
      "Mean Accuracy over 25 episodes: 87.56% ± 13.08%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate over multiple episodes\n",
    "episode_accuracies = []\n",
    "for episode in range(NUM_EPISODES):\n",
    "    support_images, support_labels, query_images, query_labels = run_episode()\n",
    "    predicted_classes = classify_images(support_images, query_images, siamese_net)\n",
    "    acc = np.mean(np.array(predicted_classes) == query_labels)\n",
    "    episode_accuracies.append(acc)\n",
    "    print(f\"Episode {episode+1}: Accuracy = {acc * 100:.2f}%\")\n",
    "\n",
    "mean_accuracy = np.mean(episode_accuracies)\n",
    "std_accuracy = np.std(episode_accuracies)\n",
    "print(f\"\\nMean Accuracy over {NUM_EPISODES} episodes: {mean_accuracy * 100:.2f}% ± {std_accuracy * 100:.2f}%\")\n",
    "\n",
    "support_images, support_labels, query_images, query_labels = run_episode()\n",
    "predicted_classes = classify_images(support_images, query_images, siamese_net)\n",
    "\n",
    "num_queries_total = query_images.shape[0]\n",
    "cols = 3\n",
    "rows = math.ceil(num_queries_total / cols)\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "axes = axes.flatten()\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < num_queries_total:\n",
    "        ax.imshow(query_images[i])\n",
    "        ax.set_title(f\"True: {query_labels[i]} | Pred: {predicted_classes[i]}\")\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MA_Arash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
